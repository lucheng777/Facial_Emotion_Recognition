{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset and split them into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath = 'C:/Users/jiaoh/Documents/GitHub/Spring2020-Project3-group2/'\n",
    "#####################   Modify above path   #########################\n",
    "os.chdir(filepath)\n",
    "\n",
    "# testpath=filepath+'/data/test_set/'\n",
    "trainpath=filepath+'/data/train_set/'\n",
    "\n",
    "# test_image_dir = testpath + \"images/\"\n",
    "# test_pt_dir = testpath + \"points/\"\n",
    "train_image_dir = trainpath + \"images/\"\n",
    "train_pt_dir = trainpath + \"points/\"\n",
    "\n",
    "import sklearn.metrics.pairwise\n",
    "def pairwise_dist(vec):\n",
    "    dist  = sklearn.metrics.pairwise_distances(vec, metric='euclidean')\n",
    "    np.fill_diagonal(dist, np.nan)\n",
    "    return dist\n",
    "def feature(fiducial_pt_list,index):\n",
    "    pairwise_dist_feature = pairwise_dist(fiducial_pt_list[index]).flatten()\n",
    "    pairwise_dist_feature = pairwise_dist_feature[~np.isnan(pairwise_dist_feature)]\n",
    "    return pairwise_dist_feature\n",
    "\n",
    "f0 = time.time()\n",
    "dataDir = train_pt_dir\n",
    "fiducial_pt_list = []\n",
    "filelist = []\n",
    "for file in os.listdir(dataDir):\n",
    "    filelist.append(file)\n",
    "filelist.sort()\n",
    "for file in filelist:\n",
    "    fiducial_pt_list.append(scipy.io.loadmat(dataDir+file))\n",
    "    l = []\n",
    "for i in range(len(fiducial_pt_list)):\n",
    "    if 'faceCoordinatesUnwarped' in fiducial_pt_list[i].keys():\n",
    "        l.append(fiducial_pt_list[i]['faceCoordinatesUnwarped'])\n",
    "    else:\n",
    "        l.append(fiducial_pt_list[i]['faceCoordinates2'])\n",
    "        \n",
    "fiducial_pt_list = l\n",
    "\n",
    "\n",
    "X = pd.DataFrame(np.zeros((2500, 6006)))\n",
    "for i in range(2500):\n",
    "    X.iloc[i,:] = np.round(feature(fiducial_pt_list, i).flatten(), 0)\n",
    "y =pd.read_csv(trainpath+'label.csv')['emotion_idx']\n",
    "f1 = time.time()-f0\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "train_x_dis,test_x_dis,train_y_dis,test_y_dis=train_test_split(X,y,test_size=0.2,random_state=3662)\n",
    "X_dis=X\n",
    "y_dis=y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GridsearchCV to determine Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  18 | elapsed:  8.2min remaining:  6.6min\n",
      "[Parallel(n_jobs=8)]: Done  18 out of  18 | elapsed: 15.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=GradientBoostingClassifier(criterion='friedman_mse',\n",
       "                                                  init=None, learning_rate=0.1,\n",
       "                                                  loss='deviance', max_depth=3,\n",
       "                                                  max_features=None,\n",
       "                                                  max_leaf_nodes=None,\n",
       "                                                  min_impurity_decrease=0.0,\n",
       "                                                  min_impurity_split=None,\n",
       "                                                  min_samples_leaf=1,\n",
       "                                                  min_samples_split=2,\n",
       "                                                  min_weight_fraction_leaf=0.0,\n",
       "                                                  n_estimators=100,\n",
       "                                                  n_iter_no_change=None,\n",
       "                                                  presort='auto',\n",
       "                                                  random_state=None,\n",
       "                                                  subsample=1.0, tol=0.0001,\n",
       "                                                  validation_fraction=0.1,\n",
       "                                                  verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=8,\n",
       "             param_grid={'learning_rate': [0.1, 0.05, 0.01], 'max_depth': [1],\n",
       "                         'n_estimators': [50, 100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# gbm_parm_gs = {\n",
    "#     'max_depth':[1] ,\n",
    "#    'n_estimators': [50,100],\n",
    "#    'learning_rate': [0.1,0.05,0.01]\n",
    "# }\n",
    "# gbm_model_gs = GradientBoostingClassifier()\n",
    "# gbm_gsearch = GridSearchCV(estimator = gbm_model_gs, \n",
    "#                        param_grid = gbm_parm_gs, \n",
    "#                        scoring ='accuracy',\n",
    "#                        cv = 3,\n",
    "#                        n_jobs = 8,\n",
    "#                        verbose=3,\n",
    "#                        return_train_score=True\n",
    "#                       )\n",
    "# gbm_gsearch.fit(train_x_dis,train_y_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset Accuracy\n",
      "0.829\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import accuracy_score  \n",
    "# print(\"Training dataset Accuracy\")\n",
    "# train_preds = gbm_gsearch.predict(train_x_dis)\n",
    "# train_accuracy = accuracy_score(train_y_dis, train_preds) \n",
    "# print(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dataset Accuracy\n",
      "0.374\n"
     ]
    }
   ],
   "source": [
    "# print(\"Testing dataset Accuracy\")\n",
    "# test_preds = gbm_gsearch.predict(test_x_dis)\n",
    "# test_accuracy = accuracy_score(test_y_dis, test_preds) \n",
    "# print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 100}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gbm_gsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use K-Fold to estimate the accuracy (41.92%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_baseline= GradientBoostingClassifier(n_estimators=100  , max_depth= 1,learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 41.92% (2.08%)\n"
     ]
    }
   ],
   "source": [
    "# kfold = KFold(n_splits=10, shuffle=True)\n",
    "# results = cross_val_score(gbm_baseline, X_dis,y_dis, cv=kfold)\n",
    "# print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the final model and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 414.886s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "gbm_baseline.fit(X_dis,y_dis)\n",
    "print(\"done in %0.3fs\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gbm_baseline.m']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib.dump(gbm_baseline,'gbm_baseline.m')#please change the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.load('C:/Users/jiaoh/Documents/GitHub/Spring2020-Project3-group2/hanbojiao_test/doc/gbm_baseline.m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
